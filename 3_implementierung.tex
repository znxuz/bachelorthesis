\section{Echtzeitanalyse - Erster Ansatz}

Nachdem die Steuerungssoftware auf zwei verschiedenen Architekturen, nämlich
FreeRTOS und Micro-ROS, entwickelt wurde, kann nun eine konkrete
Implementierung einer Methode zur Analyse der Echtzeitfähigkeit basierend auf
FreeRTOS erfolgen, um die Portabilität auf Micro-ROS zu ermöglichen. Ziel der
Analyse ist es, Informationen darüber zu gewinnen, wie lange eine bestimmte Task
oder eine bestimmte zeitkritische Funktion benötigt. Die daraus resultierenden
Daten müssen mit einer angemessenen Genauigkeit erfasst werden, um
sicherzustellen, dass die Echtzeitaspekte korrekt widergespiegelt werden.

Aufgrund von Einfachheit sowie Hardwarebeschränkungen wurde UART als
Kommunikationsschnittstelle zur Übertragung der Echtzeitdaten vom
Mikrocontroller zum Host gewählt. Mit einer theoretischen Übertragungsrate von
bis zu 12,5 Mbit/s bietet UART ausreichende Bandbreite~\cite[S.
2]{stm32_datasheet}, um die Profiling-Daten zu übertragen, ohne Überlastung zu
verursachen.

% TODO: check
der erste ansatz besteht grundsätzlich daraus, dass zu begin und am Ende der
jeweiligen Task sowie der zeitkritischen Funktion die aktuelle Zyklenzahl
aufzeichnen, um daraus die Ecvhtzeitinformationen abzuleiten.

Daraus ergibt sich als Erstes die Notwendigkeit, eine threadsichere Multi
Producer Senke, oder besser gesagt eine \ac{MPSC} Queue, zu implementieren,
welche die Echtzeitdaten kontinuierlich konsumiert und sie über UART ausgibt.
Die FreeRTOS Stream- oder Messagebuffer sind für den Fall mit mehreren Producers
nicht geeignet~\cite{FreeRTOSStreamBuffer}.

\subsection{Umsetzung mit einer Multi-Producer-Senke}

Da FreeRTOS und dementsprechend auch Micro-ROS von Natur aus multithreaded sind
und zur Echtzeitanalyse Daten von beliebiger Stelle in einem beliebigen Thread
beim Programmlauf aufgezeichnet werden, muss dabei die Threadsicherheit
gewährleistet werden, damit die zu übertragenden Daten in Form von mehreren
Bytes nicht durch Race Conditions teils überschrieben und zu unbrauchbaren Daten
werden.

% TODO: check
Die grundlegende Idee besteht darin, dass Daten von mehreren Threads direkt in
die Senke gepusht, oder besser gesagt direkt in einen internen Ringpuffer
gespeichert werden, da das Schreiben der Daten in einen statischen
Speicherpuffer wesentlich schneller ist in comparison to enqueueing the data
into a linked list, because the latter requires dynamic heap memory allocation
which costs hundreds of cycles each per syscall, where as writing to a
In-Memory-Puffer with byte alignment typischerweise $N$ zyklen kostet wobei $N$
die anzahl der Bytes ist und der schreibvorgang deterministisch ist.

Da der Speicher begrenzt ist, muss die Senke im schlimmsten Fall in der Lage
sein zu erkennen, wann sie das weitere Schreiben von Daten in den Puffer
blockieren muss, um zu verhindern, dass zuvor geschriebene, aber noch nicht
verarbeitete Daten überschrieben werden.

Aber durch die Verwendung von DMA kombiniert mit einem Interrupt ausgelöst bei
jedem Abschluss einer DMA-Übertragung kann die IO-gebundene Wartezeit zum
Konsumieren der Bytes in der Senke eliminiert werden, da in diesem Fall die
tatsächliche Ausgabe von Daten aus der Senke einfach zum Schreiben in einen
anderen In-Memory-Puffer wird, während die eigentlichen IO-Operationen auf den
DMA-Controller fern vom Prozessor ausgelagert werden. Wenn die tatsächliche IO
die Daten schnell genug überträgt, um mit den eingehenden Daten Schritt zu
halten, entsteht dabei keine Situation, in der eine Task blockiert werden muss,
dass die Senke Speicherplatz freigibt, um den Schreibvorgang fortzusetzen.

Daher wurde als der erste Ansatz mit der Umsetzung mit einer
Multi-Producer-Senke mittels DMA fortgefahren, da in diesem Fall das Schreiben
von mehreren Bytes in die Senke idealerweise nur einige Zyklen kosten und würde
sich nahezu als eine nicht-blockierende Operation vom Sicht des Prozessors oder
Threads verhalten.

\subsection{Aufbau der Multi-Producer-Senke}

Wie kurz erwähnt, besteht die Senke einfacherweise hauptsächlich aus einem
statisch vorallokierten Ringpuffer gepaart mit einem Schreib- und Lesezeiger.
Mit den beiden Zeigern wird dann ermöglicht, die Größe der bereits geschriebenen
Daten sowie der restliche verfügbare Speicherplatz zu ermitteln.

In der ersten Version der Implementierung wurde die Anzahl der Daten in der
Senke so kalkuliert, dass wenn sich der Schreibzeiger beziehungsweise der Index
im numerischen Sinne vor dem Lesezeiger befindet, ist die Anzahl von
verbrauchbaren Daten einfach die Differenz von den beiden Indexen, ansonsten
sind die zu verarbeiteten Daten von dem Lesezeiger bis zum Ende des Ringpuffers
inklusive die Daten vom Anfang des Puffers bis zum Schreibzeiger, da die Zeiger
immer auch auf die korrekte Position zeigen.

Dabei muss aber zwischen dem Fall unterschieden werden, wenn beide Zeiger
gleichzeitig auf dieselbe Position zeigen: entweder ist der Ringpuffer leer,
oder komplett voll beschrieben. Also muss der Schreiber noch wissen, ob das
aktuelle Byte bereits verbraucht wurde und deshalb überschrieben werden kann, da
er sonst in keiner Weise unterscheiden kann, ob die Senke voll ist und dann das
Schreiben verzögern soll.

Inspiriert von einem C++-Konferenzvortrag über eine
\ac{MPMC}-Warteschlange~\cite{CppCon2024LockFreeQueue}, in dem jede Position des
Datenpuffers eine eindeutige Schreibsequenznummer besitzt, diese bei der
Entnahme der Daten atomar um die Gesamtlänge des Datenpuffers $N$ erhöht,
wodurch signalisiert wird, dass die Daten an dieser Position bereits in der
Iteration $N$ verarbeitet wurden und somit einwandfrei in der nächsten Iteration
$N + 1$ vom Schreiber überschrieben werden können, was durch den Vergleich mit
der globalen Schreibsequenznummer ermöglicht wird, die ebenfalls nach jedem
Schreibvorgang atomar erhöht wird.

Für den Fall mit einer Senke mit aber nur einem einzigen Verbraucher reicht es
aus, den Zustand als \mintinline{cpp}|bool| zu speichern, der angibt, ob die
Daten an einer bestimmten Position noch verarbeitet werden müssen oder bereits
überschrieben werden können.

Um diese zusätzliche Speicheranforderung verursacht durch das explizites
Markieren des Zustands für jedes Byte in dem Puffer für die finale
Implementierung wegzuoptimieren, brauchen die Zeiger nicht mehr immer auf die
korrekte Stelle zeigen, stattdessen können sie einfach über den Puffer hinaus
zählen und bei jeder Nutzung der Zeiger deren Wert mittels einer
Modulo-Operation mit der Gesamtgröße des Puffer normalisieren, so dass sie dann
auf die tatsächliche Stelle zeigen. Dadurch kann die Kalkulation für die Anzahl
der verfügbaren Daten auf eine simple Subtraktion zwischen den beiden Zeigern
reduziert werden. Wenn die Größe des Puffers a power of two entspricht, kann die
Kosten der Modulo-Operation auf ein Zyklus reduziert werden, which is a good
trade-off and a small price to pay for eliminating the need for extra speicher
space for the Zustand.

\begin{code}
\begin{minted}{cpp}
#ifndef TSINK_CAPACITY
constexpr size_t TSINK_CAPACITY = 2048;
#endif
uint8_t sink[TSINK_CAPACITY]{};
volatile size_t read_idx = 0;
std::atomic<size_t> write_idx = 0;

size_t size() { return write_idx - read_idx; }
size_t space() { return TSINK_CAPACITY - size(); }
size_t normalize(size_t idx) { return idx % TSINK_CAPACITY; }
\end{minted}
    \captionof{listing}{Struktur der Senke}
\end{code}

\subsection{Schreibvorgang in die Senke}

Auf ARM-Architekturen sind alle Zugriffe auf im Speicher ausgerichteten Bytes,
Halbwörter (16-Bit) und Wörter (32-Bit) standardmäßig atomar und verursachen
keine Schreib-Lese-Konflikte, sowohl beim Lesen als auch beim Schreiben~\cite[S.
A3-79]{ARM_DDI0403_EE}.

Es muss jedoch sichergestellt werden, dass jeweils nur exakt ein einziger Thread
an eine Position des Ringpuffers schreiben kann, wenn mehrere Threads
gleichzeitig auf dieselbe Position zugreifen wollen.

Anbei kann eine \ac{CAS}-Operation durchgeführt werden, damit der Schreibindex
jeweils nur von einem einzigen Thread inkrementiert wird. Nach der
Inkrementierung hat somit der Thread den Anspruch, das Byte an den vorherigen
Index zu schreiben, welcher den Index erfolgreich inkrementiert hat.

\begin{code}
\begin{minted}{cpp}
bool write_or_fail(uint8_t elem) {
  auto expected = write_idx.load();
  if (expected - read_idx == TSINK_CAPACITY) return false;
  if (write_idx.compare_exchange_strong(expected, expected + 1)) {
    sink[normalize(expected)] = elem;
    return true;
  }
  return false;
}
\end{minted}
    \captionof{listing}{atomare Schreiboperation in die Senke}
\end{code}

Die Vorgehensweise ist wie folgt: zuerst wird der aktuelle Schreibindex als
lokale Variable \mintinline{cpp}|expected| zwischengespeichert und es wird damit
überprüft, ob der Puffer bereits voll ist und liefert vorzeitig zurück wenn dies
der Fall ist, sonst bedeutet es, dass die Position mit dem aktuellen Index zu
dieser Zeit noch beschreibbar ist. Danach wird die atomare CAS-Operation
durchgeführt, indem der Schreibindex mit dem zwischengespeicherten Wert
vergleicht und gleichzeitig um eins inkrementiert wird, wenn die beiden Werten
derselbe sind. Die Fähigkeit, den Vergleich und auch den darauffolgenden
Inkrement unter der Voraussetzung von Äquivalenz alle zusammen als eine atomare
Operation durchführen zu können, ermöglicht, dass nur ein Thread am Ende den
Schreibindex erfolgreich inkrementieren und folglich Daten über den
zwischengespeicherten Index schreiben kann. Dabei wird die Synchonisation also
in einer nicht-blockierenden Weise („lock-free”) garantiert.

Um das Schreiben von mehreren Bytes auch threadsicher und fehlerfrei
durchzuführen, muss ein Mutex eingesetzt werden~\cite{FreeRTOSForumPrintf}. Dies
stellt sicher im Vergleich zu einem einfach Semaphor, dass ein Thread, der ein
Mutex hält, es möglichst schnell wieder freigibt und auch niemals vom Scheduler
ausgeschlossen wird (\ref{sec:mutex}).

\begin{code}
\begin{minted}{cpp}
struct mtx_guard {
  mtx_guard() { configASSERT(xSemaphoreTake(write_mtx, portMAX_DELAY)); }
  ~mtx_guard() { configASSERT(xSemaphoreGive(write_mtx)); }
};

void write_blocking(const uint8_t* ptr, size_t len) {
  while (true) {
    if (volatile auto _ = mtx_guard{}; space() >= len) {
      for (size_t i = 0; i < len; ++i) configASSERT(write_or_fail(ptr[i]));
      return;
    }
    vTaskDelay(pdMS_TO_TICKS(1));
  }
}
\end{minted}
    \captionof{listing}{Blockierende Schreiboperation in die Senke}
\end{code}

\subsection{Lesevorgang aus der Senke}

Eine kleine, statisch allokierte FreeRTOS-Task wird erstellt, um kontinuierlich
zu versuchen, verfügbare Daten aus der Senke zu entnehmen und verarbeiten.

\begin{code}
\begin{minted}{cpp}
using consume_fn = void (*)(const uint8_t*, size_t);
consume_fn consume;

void task_impl(void*) {
  auto consume_and_wait = [](size_t pos, size_t size) static {
    if (!size) return;
    consume(sink + pos, size);
    ulTaskNotifyTake(pdFALSE, portMAX_DELAY);
  };

  while (true) {
    if (size_t sz = size(); sz) {
      auto wrap_around = ((normalize(read_idx) + sz) / TSINK_CAPACITY) *
                         normalize(read_idx + sz);
      auto immediate = sz - wrap_around;
      consume_and_wait(normalize(read_idx), immediate);
      consume_and_wait(0, wrap_around);
      read_idx += sz;
    } else {
      vTaskDelay(pdMS_TO_TICKS(1));
    }
  }
}
\end{minted}
    \captionof{listing}{Implementierung der Task zur Datenverarbeitung}
\end{code}

Als Verbrauchsfunktion \mintinline{cpp}|consume()| kann beispielsweise die
STM32-HAL-API zur Übertragung mittels DMA genutzt werden, welche einen Zeiger
zu einem Array und eine Variable als die Größe der lesbaren Daten als Parameter
einnimmt.

Hierbei wird zuerst die Größe von möglichen verfügbaren Daten vom Anfang des
Ringpuffers bis zur dem Schreibindex mathematisch kalkuliert und damit auch die
Größe vom Leseindex bis zum Ende des Puffers. Mit jedem Aufruf von
\mintinline{cpp}|consume()| wird mit \mintinline{cpp}|ulTaskNotifyTake()| darauf
gewartet, dass die aktuelle IO-Operation fertig wird und somit neue Operation
durchführen kann. Diese Vorgehensweise ist notwendig wenn
\mintinline{cpp}|consume()| beispielsweise DMA nutzt: Die DMA-API von der
STM32-HAL zur Übertragung ist möglicherweise nicht wiedereintrittsfähig und
signalisiert dabei lediglich der Hardware den gewünschten Transfervorgang und
kehrt sofort zurück \cite{HAL_UART_Transmit_DMA}. Das heißt, die Daten werden
einfach zur Verarbeitung für den DMA eingereiht, während der Programmfluss
unmittelbar fortgesetzt wird. Daher werden subsequente Aufrufe hierbei
synchronisiert.

\begin{code}
\begin{minted}{cpp}
enum struct CALL_FROM { ISR, NON_ISR };

template <CALL_FROM callsite>
void consume_complete() {
  using namespace detail;
  if constexpr (callsite == CALL_FROM::ISR) {
    static BaseType_t xHigherPriorityTaskWoken;
    vTaskNotifyGiveFromISR(task_hdl, &xHigherPriorityTaskWoken);
    portYIELD_FROM_ISR(xHigherPriorityTaskWoken);
  } else {
    xTaskNotifyGive(task_hdl);
  }
}
\end{minted}
    \captionof{listing}{Callback-Funktion für die Task-Notification}
\end{code}

Erst nachdem die Task-Notifikation durch den Aufruf von
\mintinline{cpp}|consume_complete()| empfangen wird, beispielsweise von einer
\ac{ISR}, die durch die DMA-Hardware nach Abschluss der Übertragung ausgelöst
wird, wird die Task wieder entblockt um weitere IO-Operationen zu beauftragen.

\subsection{Nutzung der Senke mit DMA}

Um diese Senke mit DMA und aktiviertem Daten-Cache zu verwenden, muss zunächst
die Interrupt-Callback \mintinline{cpp}|HAL_UART_TxCpltCallback()| definiert
werden, die bei Abschluss jedes DMA-Transfers ausgelöst wird.

Die Initialisierungsfunktion der Senke ist dann aufzurufen, welche einen
Funktionszeiger vom Typ \mintinline{cpp}|consume_fn| zur Verarbeitung von
Daten~(\ref{code:cache_clean}) von Daten sowie eine Priorität für die interne
Verbraucher-Task als Argumente entgegennehmen.

\begin{code}
\begin{minted}{cpp}
void HAL_UART_TxCpltCallback(UART_HandleTypeDef* huart) {
  if (huart->Instance == huart3.Instance)
    tsink::consume_complete<tsink::CALL_FROM::ISR>();
}

void main() {
  auto tsink_consume_dma = [](const uint8_t* buf, size_t size) static {
    auto flush_cache_aligned = [](uintptr_t addr, size_t size) static {
      constexpr auto align_addr = [](uintptr_t addr) { return addr & ~0x1F; };
      constexpr auto align_size = [](uintptr_t addr, size_t size) {
        return size + ((addr) & 0x1F);
      };

      SCB_CleanDCache_by_Addr(reinterpret_cast<uint32_t*>(align_addr(addr)),
                              align_size(addr, size));
    };

    flush_cache_aligned(reinterpret_cast<uintptr_t>(buf), size);
    HAL_UART_Transmit_DMA(&huart3, buf, size);
  };
  tsink::init(tsink_consume_dma, osPriorityAboveNormal);
}
\end{minted}
    \captionof{listing}{Initialisierung der Senke mit DMA}
\end{code}

\subsection{Nutzung der Senke mit blockierender IO}

Ähnlich wie bei der Initialisierung über DMA, entfällt hier aber der
Interrupt-Callback, und die Funktion zur Datenverarbeitung wird durch die
Verwendung der blockierenden API ohne Leerung von Cache vereinfacht, da ohne DMA
keine manuelle Sicherstellung der Cache-Kohärenz notwendig ist.

\begin{code}
\begin{minted}{cpp}
int main() {
  auto tsink_consume = [](const uint8_t* buf, size_t size) static {
    HAL_UART_Transmit(&huart3, buf, size, HAL_MAX_DELAY);
    tsink::consume_complete<tsink::CALL_FROM::NON_ISR>();
  };

  tsink::init(tsink_consume, osPriorityAboveNormal);
}
\end{minted}
    \captionof{listing}{Initialisierung der Senke mit blockierender IO}
\end{code}

\subsection{Benchmark}

Ein Benchmark für die Senke wurde entwickelt, um deren Leistung unter paralleler
Last zu testen. Der Benchmark lässt eine Anzahl von
\mintinline{text}|BENCHMARK_N = 5| Threads gleichzeitig laufen, die jeweils eine
Anzahl von \mintinline{cpp}|iteration = 5000| Nachrichten mit ca. 80 Charaktern
nach Formatierung hintereinander über die Senke ausgeben.

Nach Abschluss des Benchmarks werden die gemessenen Zeiten und die
Laufzeitstatistiken der jeweiligen Task ausgegeben.

\begin{minipage}[t]{0.5\textwidth}
    \begin{code}
        \begin{minted}[linenos=false]{cpp}
time in ms: 8543
time in ms: 8728
time in ms: 9196
time in ms: 9342
time in ms: 9571
===================================
Task            Time            %%
print_bench     1               <1%
Tmr Svc         0               <1%
IDLE            72753           76%
benchmark       4363            4%
benchmark       4377            4%
benchmark       4257            4%
benchmark       4443            4%
benchmark       4238            4%
tsink           351             <1%
    \end{minted}
        \captionof{listing}{Benchmark DMA}
    \end{code}
\end{minipage}
\hfill
\begin{minipage}[t]{0.5\textwidth}
    \begin{code}
        \begin{minted}[linenos=false]{cpp}
time in ms: 10964
time in ms: 11016
time in ms: 11285
time in ms: 11379
time in ms: 11405
===================================
Task            Time            %%
print_bench     0               <1%
Tmr Svc         0               <1%
IDLE            0               <1%
benchmark       3624            3%
benchmark       3637            3%
benchmark       3623            3%
benchmark       3644            3%
benchmark       3631            3%
tsink           94876           83%
    \end{minted}
        \captionof{listing}{Benchmark mit blockierender IO}
    \end{code}
\end{minipage}

Die Ausgabe enthält zwei verschiedene Zeitmessungen für den Benchmark. Die erste
Messung erfasst die Zeitspanne vom Start des jeweiligen Threads bis zu dessen
Beendigung. Die zweite Messung bezieht sich auf die
FreeRTOS-Laufzeitstatistiken, die durch \mintinline{cpp}|vTaskGetRunTimeStats()|
formattiert ausgegeben werden. Diese liefern die absolute akkumulierte Zeit für
jede Task, die im Zustand „Running” verbracht hat, sowie deren prozentualen
Anteil an der Gesamtlaufzeit \cite{freertos_runtime_stats}.

Der Benchmark zeigt, dass asynchrone Übertragung per DMA die Gesamtlaufzeit des
Benchmark-Prozesses im Vergleich zur IO-gebundenen Variante um etwa $16\,\%$
verringerte, während gleichzeitig die IO-gebundene Zeit freigegeben wurde,
sodass sie von anderen Aufgaben genutzt werden kann.

Ebenso kann abgeleitet werden, dass durch die Verwendung von DMA die
Datenübertragungsrate nahezu das vorkonfigurierte Maximum der Baudrate von
$2.000.000\text{ bps}$ erreicht wurde. Insgesamt wurden $1.908.759$ Bytes
übertragen, dabei hat ein UART-Byte-Frame eine standardmäßige Wortlänge von 8
Bit hat, inklusive je 1 Start- und 1 Stopp-Bit, ohne Paritätsbit.

\begin{align*}
    1.908.355\text{\,B} \times 10\text{\,b per Frame} =
    19.083.550 \text{\,b} = \text{Gesamte Bits}
\end{align*}

Teilt man dies durch die gesamte Übertragungszeit, ergibt sich die effektive
Bitrate sowie der prozentuale Anteil im Vergleich zur maximalen Baudrate:

\begin{align*}
    \text{Bitrate bei DMA} =
    \frac{19.083.550\text{\,b}}{9,571\text{\,s}} \approx
    1.993.893,01 \text{\,bps} \\
    \quad \Rightarrow 99,70\,\%\text{ des Maximums} \\
    \\
    \text{Bitrate bei blockierender IO} =
    \frac{19.083.550\text{\,b}}{11,405\text{\,s}} \approx
    1.673.261,73 \text{\,bps} \\
    \quad \Rightarrow 83,66\,\% \text{ des Maximums}
\end{align*}

Der Code für die Senke sowie den Benchmark befinden sich in den
Repositorys~\cite{freertos_threadsafe_sink, freertos_tsink_benchmark}.

\subsection{Implementierung des Messverfahrens}

Nachdem die threadsichere Datenausgabe implementiert wurde, kann nun die Frage
geklärt werden, wie die Dauer eines beliebigen Funktionsaufrufs oder einer Task
von der Zuweisung durch den Scheduler bis zum Abschluss gemessen werden kann.

\subsubsection{Aktivierung der DWT}

Wie im vorherigen Abschnitt erläutert \ref{sec:dwt}, stellt die DWT einen
geeigneten Ansatz zur Generierung von Echtzeitdaten in Form von Zyklenzahl dar.
Sie ist standardmäßig auf Cortex-M7-Prozessoren verfügbar und kann durch die
folgenden Konfigurationsschritte aktiviert werden:

\begin{code}
\begin{minted}{cpp}
void enable_dwt() {
  CoreDebug->DEMCR |= CoreDebug_DEMCR_TRCENA_Msk;
  DWT->LAR = 0xC5ACCE55;  // software unlock
  DWT->CYCCNT = 1;
  DWT->CTRL |= DWT_CTRL_CYCCNTENA_Msk;
}
\end{minted}
    \captionof{listing}{Aktivierung der DWT \cite{StackOverflow_DWT_Activation}}
\end{code}

Danach kann die aktuelle Zyklenzahl direkt über \mintinline{cpp}|DWT->CYCCNT|
ausgelesen werden.

\subsection{Aufzeichnung von Zyklenstempeln}

Drei wesentliche Informationen werden bei der Aufzeichnung von Zyklenstempeln
erfasst: der Identifikator der zugehörigen Task oder Funktion, der aktuelle
Zyklenzahl und ein Marker, der angibt, ob der Zyklenstempel den Beginn oder das
Ende einer Dauer markiert. Diese Daten werden in einem Strukturtyp gespeichert.

\begin{code}
\begin{minted}{cpp}
struct cycle_stamp {
  const char* name;
  size_t cycle;
  bool is_begin;
};
\end{minted}
    \captionof{listing}{Definition des Zyklenstempels}
\end{code}

% TODO: continue

Da das Schreiben in die Senke per se nicht „lock-free“ ist, kann es nicht direkt
in einer ISR zur Erstellung eines Zyklenstempels aufgerufen werden. Stattdessen
müssen die Daten erst in einen temporären Puffer reingeschrieben werden.

\begin{code}
\begin{minted}{cpp}
inline constexpr size_t STAMP_BUF_SIZE = 512;

inline std::array<cycle_stamp, STAMP_BUF_SIZE> stamps{};
volatile inline size_t stamp_idx = 0;
volatile inline bool stamping_enabled = 0;
\end{minted}
    \captionof{listing}{Globaler Temporärpuffer mit dessen Schreibindex und
    Aktivierungsflag}
\end{code}

Die Funktion zur Aufzeichnung eines Zyklenstempels ist wie folgt definiert:

\begin{code}
\begin{minted}{cpp}
template <bool from_isr>
inline void stamp(const char* name, bool is_begin) {
  struct disable_isr_guard {
    disable_isr_guard() { taskENTER_CRITICAL(); }
    ~disable_isr_guard() { taskEXIT_CRITICAL(); }
  };

  if constexpr (!from_isr) volatile auto _ = disable_isr_guard();

  stamps[stamp_idx] = {name, DWT->CYCCNT, is_begin};
  stamp_idx = (stamp_idx + 1) % stamps.size();
}
\end{minted}
    \captionof{listing}{Funktion zur Aufzeichnung von Zyklenstempeln}
\end{code}

Innerhalb der Funktion ist die Definition eines Typs \mintinline{cpp}|struct
disable_isr_guard| zu finden, welcher dafür sorgt, dass diese Funktion, wenn
auch von einem nicht-ISR-Kontext aufgerufen wird, den gesamten Funktionsblock
beziehungsweise die Aufzeichnung als ein kritischer Abschnitt ausführt.

Das Konzept von \ac{RAII} wird hierbei angewendet, um beim Konstruieren eines
Objekts von diesem Typ automatisch die Funktion
\mintinline{cpp}|taskENTER_CRITICAL()| sowie bei dessen Dekonstruktion – beim
Verlassen der Funktion \mintinline{cpp}|stamp()| –
\mintinline{cpp}|taskEXIT_CRITICAL()| aufzurufen und dadurch einen kritischen
Abschnitt bilden. Laut des ISO-C++-Standards wird der Aufruf von Destruktoren
mit „Side Effects”\footnotemark{} nicht durch Optimierung eliminiert und erfolgt
garantiert am Ende des Ausführungsblocks, selbst wenn das Objekt nicht genutzt
zu sein scheint~\cite[§6.7.5.4 Abs. 3]{iso_iec_14882_2020}, und zwar in der
umgekehrten Reihenfolge, wie die Objekte kreiert worden sind
\cite{isocpp_dtor_order}.

\footnotetext{Zu „Side Effects” zählen unter anderem Lese- oder Schreibzugriffe
auf ein Objekt, das als \mintinline{cpp}|volatile| gekennzeichnet ist, sowie
Modifikationen eines Objekts.~\cite{cppreference_eval_order}}

Mit einem kritischen Abschnitt kann somit sichergestellt werden, dass zwischen
der Lesezugriff auf den Schreibindex \mintinline{cpp}|stamp_idx| und dessen
Inkrementierung nicht durch Kontextwechsel unterbrochen wird, so dass die
geschriebene Daten nicht unmittelbar vor der Inkrementierung des Schreibindexes
von anderen Threads überschrieben werden.

Mittels einer booleschen Variable als Template-Argument, kombiniert mit
\mintinline{cpp}|if constexpr|, kann zur Übersetzungszeit festgelegt werden, ob
das Objekt erzeugt oder weggelassen werden soll. Konkret werden dabei zwei
unterschiedliche Versionen dieser Funktion durch den Compiler generiert, wodurch
das Branching zur Laufzeit vollständig eliminiert wird
\cite{cppreference_constexpr_if}. Je nachdem, ob der Boolean falsch oder wahr
ist, wird die entsprechende Version mit oder ohne die Erzeugung des Objekts für
den kritischen Abschnitt aufgerufen.

\subsubsection{Aufzeichnung in Anwendungscode}

Auch hier kann erneut auf die RAII zurückgegriffen werden, da sie es ermöglicht,
Code automatisch zu Beginn und am Ende eines beliebigen, kontinuierlichen
Abschnitts im Anwendungscode auszuführen.

\begin{code}
\begin{minted}{cpp}
struct cycle_stamp_raii {
  cycle_stamp_raii(const char* name) : name{name} {
    if (stamping_enabled) stamp<false>(name, true);
  }
  ~cycle_stamp_raii() {
    if (stamping_enabled) stamp<false>(name, false);
  }

  const char* name;
};
\end{minted}
    \captionof{listing}{Definition des Strukturtyps zur manuellen Aufzeichnung}
\end{code}

Um die Dauer einer Funktion zu messen, kann einfach ein Objekt dieses Typs zu
Beginn der Funktion definiert werden; dasselbe gilt auch für einen spezifischen
Code-Abschnitt.

\begin{code}
\begin{minted}{cpp}
void func()
{ // --> stamped at the start of this function
  volatile cycle_stamp_raii _{"func"};
  { // --> stamped at the start of this code block
    volatile cycle_stamp_raii _{"code block"};
  } // --> stamped when this block ends
} // --> stamped when this function goes out of scope
\end{minted}
    \captionof{listing}{Beispielnutzung des RAII-Strukturtyps}
\end{code}

\subsubsection{Aufzeichnung beim Kontextwechsel}

FreeRTOS bietet eine Vielzahl von Makros, die beim Kontextwechsel, genauer
gesagt zu Beginn und Abschluss jedes Zeitabschnitts (engl. Time Slice) der
aktuell laufenden Task, als ISR-Callbacks aufgerufen werden
können.(\ref{sec:trace_hooks}). Das Makro
\mintinline{cpp}|traceTASK_SWITCHED_IN()| wird aufgerufen, nachdem eine Task zum
Ausführen ausgewählt wurde. \mintinline{cpp}|traceTASK_SWITCHED_OUT()| wird
aufgerufen, unmittelbar bevor eine neue Task ausgewählt wird. An diesen
Zeitpunkten enthält \mintinline{cpp}|pxCurrentTCB| (der interne
Task-Control-Block-Struktur von FreeRTOS) die Metadaten der aktuellen Task,
wodurch der Nutzer die Chance hat, direkt darauf zuzugreifen.
(\cite{freertos_rtos_trace_hooks})

Da die Definitionen solcher Makros immer vor der Einbindung der
\mintinline{text}|FreeRTOS.h| erfolgen müssen, können sie einfachheitshalber
am Ende der \mintinline{text}|FreeRTOSConfig.h| definiert werden.

\begin{code}
\begin{minted}{cpp}
void task_switched_isr(const char* name, uint8_t start);
#define traceTASK_SWITCHED_IN() \
    task_switched_isr(pxCurrentTCB->pcTaskName, 1)
#define traceTASK_SWITCHED_OUT() \
    task_switched_isr(pxCurrentTCB->pcTaskName, 0)
\end{minted}
    \captionof{listing}{Konkrete Definition der Trace Hook Makros}
\end{code}

Hierbei werden die Makros jeweils als ein Aufruf der Funktion
\mintinline{cpp}|task_switched_isr()| mit dem Namen der aktuellen Task
\mintinline{cpp}|pcTaskName| sowie einen boolesche Start/End-Marker, definiert
als \mintinline{cpp}|uint8_t| um das Einbinden von \mintinline{cpp}|<stdbool.h>|
zu sparen, definiert.

Das Feld \mintinline{cpp}|uxTaskNumber| vom Typ \mintinline{cpp}|unsigned long|
aus dem \mintinline{cpp}|pxCurrentTCB|-Objekt, das eigentlich speziell zur
Task-Identifizierung für Drittanbieter-Softwares konzipiert
ist~\cite{freertos_task_c_410}, kann in dem Falle auch als möglicherweise der
leichtgewichtigste Identifikator genutzt werden. Da das Ausgeben des
menschenlesbaren Namens keinen Bottleneck bei der IO-Übertragung verursacht und
man in der Nachbearbeitung nicht jeden generierten Zyklenstempel manuell mit der
zugehörigen Task- oder Funktionsname abgleichen muss, wird hier
einfachheitshalber auf den Namen entschieden.

\begin{code}
\begin{minted}{cpp}
void task_switched_isr(const char* name, uint8_t start) {
  if (!stamping_enabled) return;

  stamp<true>(name, start);
  ctx_switch_cnt += 1;
}
\end{minted}
    \captionof{listing}{Funktion zur Zyklenstempelgenerierung beim
    Kontextwechsel}
\end{code}

Die Funktion überprüft zunächst, ob die Aufzeichnung beim Kontextwechsel
durchgeführt werden soll, und ruft anschließend \mintinline{cpp}|record()| auf,
falls dies der Fall ist. Nebenbei wird ein Zähler inkrementiert, der die
akkumulierte Anzahl der Kontextwechsel repräsentiert.

% TODO

Mit den Trace Hooks, die zu Beginn und Abschluss jedes Task-Zeitabschnitts
ausgelöst werden, und dem RAII-Strukturtyp zur manuellen Generierung von
Zyklenstempeln
